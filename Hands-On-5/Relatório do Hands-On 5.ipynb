{"cells":[{"cell_type":"markdown","metadata":{"id":"xXATwZ8VxZG7","tags":[]},"source":["# Relatório Hands-On-5\n","\n","Fernando Antonio Marques Schettini $^1$, Gabriel Mascarenhas Costa de Sousa$^2$, Jadson Nobre das Virgens$^2$\n","\n","$^1$ Curso de Engenharia de Computação - Centro universitário SENAI CIMATEC, Salvador, Bahia, Brazil  \n","\n","$^2$ Curso de Sistemas de Informação - Universidade do Estado da Bahia, Salvador, Bahia, Brazil"]},{"cell_type":"markdown","metadata":{"id":"B5PcGb-Pt0_j"},"source":["# Resumo\n"]},{"cell_type":"markdown","metadata":{"id":"orxl23v_xqVe"},"source":["Este é o relátorio das atividades realizadas durante a execução da prática Hands-On-5 [1] . O relatório foi feito como atividade avaliativa da matéria Fundamentos de Programação Paralela, lecionada no centro universitário SENAI CIMATEC.\n","\n","Dando continuidade às praticas de Hands-On, esse está dividido em três atividades relacionadas ao uso do modelo MPI[2] (Message Passing Interface), para o entendimento do conceito de paralelismo com memoria distribuída"]},{"cell_type":"markdown","metadata":{"id":"dGlWZ-SlQObx"},"source":["# Introdução"]},{"cell_type":"markdown","metadata":{"id":"ngiuqnkbvTdx"},"source":["Nesse Hands-On vamos trabalhar com três atividades que introduzem bem a ideia de distruibuição de tarefas entre processos.\n","Para isso, usaremos os conceitos de MPI ensinados em sala e através dos materiais de apoio compartilhados para:\n","\n","1. Realizar diferentes operações matemáticas basicas em um mesmo vetor de inteiros\n","2. Solucionar equações algébricas de terceiro gral de forma dinâmica, onde cada variável pode ser inserida pelo usuário durante a execução do programa.\n","3. Realizar operações em uma matriz quadrada alterando sua diagonal principal, superdiagonal e subdiagonal, a partir de valores de uma variável \"***K***\" predefinidos."]},{"cell_type":"markdown","source":["Podemos ver então que temos já definidas as três operações que iremos aplicar: Soma, subtração e multiplicação.\n","\n","A ideia aqui é que iremos dividir essas tarefas entre diferentes processos, onde um estará responsável por inicializar o vetor e delegar as tarefas aos outros três."],"metadata":{"id":"EqyqYZMsWy0G"}},{"cell_type":"markdown","source":["# Tarefa 1"],"metadata":{"id":"uuO60awcZgFq"}},{"cell_type":"markdown","source":["A primeira atividade é bem simples, apenas para demonstrar a execução de um hello world utilizando as tecnicas misturadas de OpenMP com MPI. O código se deu da seguinte forma:"],"metadata":{"id":"_MRuE8vyZo4d"}},{"cell_type":"code","source":["%%writefile hybrid.c\n","\n","#include <mpi.h>\n","#include <omp.h>\n","#include <stdio.h>\n","#include <stdlib.h>\n","int main( int argc, char *argv[])\n","{\n","  int nthreads,nprocs,idpro,idthr;\n","  int namelen;\n","  char processor_name[MPI_MAX_PROCESSOR_NAME];\n","  MPI_Init(&argc,&argv);\n","  MPI_Comm_size(MPI_COMM_WORLD,&nprocs);\n","  MPI_Comm_rank(MPI_COMM_WORLD,&idpro);\n","  MPI_Get_processor_name(processor_name,&namelen);\n","  #pragma omp parallel private(idthr) firstprivate(idpro,processor_name)\n","  {\n","    idthr = omp_get_thread_num();\n","    printf(\"Hello World thread %d,From %d processor %s\\n\",idthr,idpro,processor_name);\n","  }\n","\n","  MPI_Finalize();\n","}"],"metadata":{"id":"4oa0PUDuWiCW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%shell\n","sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev libgtk2.0-dev\n","sudo apt-get install libomp-dev gcc"],"metadata":{"id":"oXhqkbOOhIKN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%shell\n","mpicc hybrid.c -o hybryd -fopenmp\n","OMP_NUM_THREADS=8 mpirun --allow-run-as-root -np 2 ./hybryd"],"metadata":{"id":"6RQedC3HjdDr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como pudemos observar, foram abertos dois processos onde cada um deles abriu oito threads e todos, um por um, imprimiram na tela um \"hello world\" indicando seus respectivos números de thread/processo."],"metadata":{"id":"QCJ17kpPrHQf"}},{"cell_type":"markdown","source":["# Tarefa 2"],"metadata":{"id":"nzyCnNBYbGSK"}},{"cell_type":"markdown","source":["A segunda atividade segue o mesmo principio de distribuição de tarefas, dessa vez para o calculo de uma função algebrica de terceiro gral, onde cada processo ficará encarregado de  calcular um pedaço da equação.\n","\n","Começamos com os mesmos métodos de ***Init***, definição de ***size*** e de ***rank***. Lembrando sempre de incluir a lib mpi.h e declarar as demais variáveis necessárias para o programa."],"metadata":{"id":"hFwSadAFbJ-4"}},{"cell_type":"code","source":["#include <stdio.h>\n","#include <math.h>\n","#include <mpi.h>\n","\n","int main (int argc, char **argv){\n","\n","  double coefficient[4], total, x, p[3], vars[3], result, w1[2], w2[2], w3[3];\n","  char c;\n","  int numOfProc, id, tag1 = 10, tag2 = 20, tag3 = 30;\n","\n","  MPI_Init(&argc, &argv);\n","  MPI_Comm_size(MPI_COMM_WORLD, &numOfProc);\n","  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n","  MPI_Status status;\n","  ..."],"metadata":{"id":"B5587rLzy7HH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como dessa vez, os processos irão receber valores diferentes com os quais trabalhar, também escolhemos criar tags diferentes para cada processo, apenas para ajudar na distinção dos comandos. Depois que fazemos essa parte inicial podemos continuar para a divisão de trabalho, sempre começando pelo rank do processo mestre.\n","\n","Aqui o processo 0 começa imprimindo a fórmula da equação na tela e pedindo para que o usuário insira cada variavel relacionada a ela. Depois que todos os valores das variáveis são inseridos, os vetores com os dados que devem ser enviados a cada processo são separados e enviados um por um.\n","\n","Por fim, o mestre aguarda o retorno dos valores já calculados por cada processo e os concatena no resultado final, o impimindo na tela."],"metadata":{"id":"L4PldtG60kM2"}},{"cell_type":"code","source":["  ...\n","  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n","  MPI_Status status;\n","\n","  switch(id){\n","    case 0:\n","      printf (\"\\nf(x) = a*x^3 + b*x^2 + c*x + d\\n\");\n","\n","      for(c = 'a'; c < 'e'; c++) {\n","        printf (\"\\nEnter the value of the 'constants' %c:\\n\", c);\n","        scanf (\"%lf\", &coefficient[c - 'a']);\n","      }\n","\n","      printf(\"\\nf(x) = %lf*x^3 + %lf*x^2 + %lf*x + %lf\\n\", coefficient[0], coefficient[1], coefficient[2], coefficient[3]);\n","\n","      printf(\"\\nEnter the value of 'x':\\n\");\n","      scanf(\"%lf\", &x);\n","      \n","      w1[0] = coefficient[0];\n","      w2[0] = coefficient[1];\n","      w3[0] = coefficient[2];\n","      w3[1] = coefficient[3];\n","      w1[1] = w2[1] = w3[2] = x;\n","      \n","      MPI_Send(&w1, 2, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n","      MPI_Send(&w2, 2, MPI_DOUBLE, 2, tag2, MPI_COMM_WORLD);\n","      MPI_Send(&w3, 3, MPI_DOUBLE, 3, tag3, MPI_COMM_WORLD);\n","      \n","      MPI_Recv(&p[0], 1, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD, &status);\n","      MPI_Recv(&p[1], 1, MPI_DOUBLE, 2, tag2, MPI_COMM_WORLD, &status);\n","      MPI_Recv(&p[2], 1, MPI_DOUBLE, 3, tag3, MPI_COMM_WORLD, &status);\n","      \n","      printf(\"\\nf(%lf) = %lf\\n\", x, p[0]+p[1]+p[2]);\n","      break;\n","      ..."],"metadata":{"id":"A7OqG53A1D1D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A atividade executada pelos processos filhos segue um esquema parecido com o da primeira tarefa, onde cada processo calcula uma parte independente da equação e a retorna para o processo mestre.\n","\n","Ao final, também encerramos o MPI com o ***MPI_Finalize***."],"metadata":{"id":"HICGLHKf2RAS"}},{"cell_type":"code","source":["      ...  \n","      printf(\"\\nf(%lf) = %lf\\n\", x, p[0]+p[1]+p[2]);\n","      break;\n","    \n","    case 1:\n","      printf(\"case 1\");\n","      MPI_Recv(&vars, 2, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n","      //result = vars[0] * pow(vars[1], 3);\n","      result = vars[0] * vars[1] * vars[1] * vars[1];\n","      MPI_Send(&result, 1, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD);\n","      break;\n","    case 2:\n","      MPI_Recv(&vars, 2, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD, &status);\n","      //result = vars[0] * pow(vars[1], 2);\n","      result = vars[0] * vars[1] * vars[1];\n","      MPI_Send(&result, 1, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n","      break;\n","    case 3:\n","      MPI_Recv(&vars, 3, MPI_DOUBLE, 0, tag3, MPI_COMM_WORLD, &status);\n","      result = vars[0] * vars[2] + vars[1];\n","      MPI_Send(&result, 1, MPI_DOUBLE, 0, tag3, MPI_COMM_WORLD);\n","      break;\n","  }\n","\n","  MPI_Finalize();\n","  return 0;\n","}"],"metadata":{"id":"lAGnIG-92g6j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tarefa 3"],"metadata":{"id":"WKytTFFKbIUa"}},{"cell_type":"markdown","source":["Por fim, a terceira atividade nos apresenta uma matriz quadrada onde os valores da diagonal principal, superdiagonal e subdiagonal devem sofrer alterações com base em valores de variaveis \"***K***\" pré-definidos. A solução dessa atividade se assemelha bastante à primeira parte desse Hands-On, pois enviaremos a mesma matriz inicializada pelo processo mestre, para cada um dos outros processos, apenas alterando a operação que será executada por cada um.\n","\n","Começamos então com os mesmos métodos de anteriormente, além de aproveitar uma função de impressão de matriz já codificada pelo professor."],"metadata":{"id":"6Uowebg6bQaY"}},{"cell_type":"code","source":["#include <stdio.h>\n","#include <mpi.h>\n","#define ORDER 4\n","\n","void printMatrix (int m[][ORDER]) {\n","  int i, j;\n","  for(i = 0; i < ORDER; i++) {\n","    printf (\"| \");\n","    for (j = 0; j < ORDER; j++) {\n","      printf (\"%3d \", m[i][j]);\n","    }\n","    printf (\"|\\n\");\n","  }\n","  printf (\"\\n\");\n","}\n","\n","int main (int argc, char **argv){\n","\n","  int k[3] = {100, 200, 300};\n","  int matrix[ORDER][ORDER], rcv1[ORDER][ORDER], rcv2[ORDER][ORDER], rcv3[ORDER][ORDER], size = ORDER*ORDER;\n","  int i, j, p, numProc, rank, tag = 10;\n","  \n","  MPI_Init(&argc, &argv);\n","  MPI_Comm_size(MPI_COMM_WORLD, &numProc);\n","  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n","  MPI_Status status;\n","  ..."],"metadata":{"id":"OoyOxsrK4U-0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Passamos então para o rank 0 que fica responsável por inicializar a matriz e a enviar para os outros processos, como de costume. Mas dessa vez, na hora da recepção, faremos atribuições distintas com base nas matrizes recebidas de cada processo.\n","\n","Nesse caso, cada rank ficou responsável por uma diagonal, sendo elas a ***principal***, a ***superdiagonal*** e a ***subdiagonal***, da matriz quadrada. Com isso, a mensagem de resposta de cada processo contem uma matriz identica à original, com exceção da diagonal a qual o mesmo ficou encarregado.\n","\n","Quando a resposta dos ranks filhos é recebida, o processo mestre roda um único loop passando por cada posição da matriz e apenas reatribui os novos valores às suas respectivas diagonais."],"metadata":{"id":"E6cBqMxe4l2v"}},{"cell_type":"code","source":["  ...\n","  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n","  MPI_Status status;\n","\n","  switch(rank){\n","    case 0:\n","      for(i = 0; i < ORDER; i++) {\n","        for(j = 0; j < ORDER; j++) {\n","          if( i == j )\n","            matrix[i][j] = i + j +1;\n","          else if(i == (j + 1)) {\n","            matrix[i][j] = i +  j + 1;\n","            matrix[j][i] = matrix[i][j];\n","          } else\n","           matrix[i][j] = 0;\n","        }\n","      }\n","      printMatrix(matrix);\n","      for(p = 1; p < numProc; p++){\n","        MPI_Send(&matrix, size, MPI_INT, p, tag, MPI_COMM_WORLD);\n","      }\n","      MPI_Recv(&rcv1, size, MPI_INT, 1, tag, MPI_COMM_WORLD, &status);\n","      MPI_Recv(&rcv2, size, MPI_INT, 2, tag, MPI_COMM_WORLD, &status);\n","      MPI_Recv(&rcv3, size, MPI_INT, 3, tag, MPI_COMM_WORLD, &status);\n","      for(i = 0; i < ORDER; i++){\n","        matrix[i][i] = rcv1[i][i];\n","        matrix[i + 1][i] = rcv2[i + 1][i];\n","        matrix[i][i + 1] = rcv3[i][i + 1];    \n","      }\n","      printMatrix(matrix);\n","      break;\n","      ..."],"metadata":{"id":"yzMs7vdJ602s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para finalizar, criamos um case ***default*** para os ranks filhos, ja que todos recebem a mesma matriz. Um único loop roda em cada processo e apenas nas posições designadas a eles, os mesmos executam a operação de soma com os respecivos valores de ***K***, retornando a matriz modificada para o processo mestre logo após isso.\n","\n","Por fim, mais uma vez, encerramos o MPI."],"metadata":{"id":"79wwE5ly7Nrk"}},{"cell_type":"code","source":["      ...\n","      printMatrix(matrix);\n","      break;\n","\n","    default:\n","      MPI_Recv(&matrix, size, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);\n","\n","      for(i = 0; i < ORDER; i++){\n","        switch(rank){\n","          case 1:\n","            matrix[i][i] += k[0];\n","            break;\n","          case 2:\n","            matrix[i + 1][i] += k[1];\n","            break;\n","          case 3:\n","            matrix[i][i + 1] += k[2];\n","            break;\n","        }\n","      }\n","      MPI_Send(&matrix, size, MPI_INT, 0, tag, MPI_COMM_WORLD);\n","      break;\n","  }\n","\n","  MPI_Finalize();\n","  return 0;\n","}"],"metadata":{"id":"XrGTCB_l7xgI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EkecWU4KKvo"},"source":["# Conclusões"]},{"cell_type":"markdown","metadata":{"id":"EOXpKoRrJ5sm"},"source":["Podemos notar claramente a semelhança entre as três atividades praticadas por esse Hands-On, o que ajuda bastante a se acostumar com os métodos e variáveis utilizados pelo MPI. Apesar de semelhantes, cada atividade exigiu uma estratégia diferente para o envio e recebimento das mensagens por cada processo, o que também auxilia no aprendizado de uso da ferramenta."]},{"cell_type":"markdown","metadata":{"id":"qbxAR_nZ3ey8"},"source":["# Referencias"]},{"cell_type":"markdown","metadata":{"id":"eHs1Te143ezA"},"source":["[1] M. Boratto. Hands-On Supercomputing with Parallel Computing. Available: https://github.com/\n","muriloboratto/Hands-On-Supercomputing-with-Parallel-Computing. 2022.\n","\n","[2] Forum, Message Passing Interface. MPI: A Message-Passing Interface Standard. University of Tennessee,\n","1994, USA.\n","\n","[3] B. Chapman, G. Jost and R. Pas. Using OpenMP: Portable Shared Memory Parallel Programming. The\n","MIT Press, 2007, USA."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/jupyter-papers/mock-paper/blob/master/FerroicBlocks_mockup_paper_v3a.ipynb","timestamp":1659115650450}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","metadata":{"id":"xXATwZ8VxZG7","tags":[]},"source":["# Relatório Hands-On-3\n","\n","Fernando Antonio Marques Schettini $^1$, Gabriel Mascarenhas Costa de Sousa$^2$, Jadson Nobre das Virgens$^2$\n","\n","$^1$ Curso de Engenharia de Computação - Centro universitário SENAI CIMATEC, Salvador, Bahia, Brazil  \n","\n","$^2$ Curso de Sistemas de Informação - Universidade do Estado da Bahia, Salvador, Bahia, Brazil"]},{"cell_type":"markdown","metadata":{"id":"B5PcGb-Pt0_j"},"source":["# Resumo\n"]},{"cell_type":"markdown","metadata":{"id":"orxl23v_xqVe"},"source":["Este é o relátorio das atividades realizadas durante a execução da prática Hands-On-3 [1] . O relatório foi feito como atividade avaliativa da matéria Fundamentos de Programação Paralela, lecionada no centro universitário SENAI CIMATEC.\n","\n","Dando continuidade às praticas de Hands-On, esse está dividido em três atividades relacionadas ao uso do modelo MPI[2] (Message Passing Interface), para o entendimento do conceito de paralelismo com memoria distribuída"]},{"cell_type":"markdown","metadata":{"id":"dGlWZ-SlQObx"},"source":["# Introdução"]},{"cell_type":"markdown","metadata":{"id":"ngiuqnkbvTdx"},"source":["Nesse Hands-On vamos trabalhar com três atividades que introduzem bem a ideia de distruibuição de tarefas entre processos.\n","Para isso, usaremos os conceitos de MPI ensinados em sala e através dos materiais de apoio compartilhados para:\n","\n","1. Realizar diferentes operações matemáticas basicas em um mesmo vetor de inteiros\n","2. Solucionar equações algébricas de terceiro gral de forma dinâmica, onde cada variável pode ser inserida pelo usuário durante a execução do programa.\n","3. Realizar operações em uma matriz quadrada alterando sua diagonal principal, superdiagonal e subdiagonal, a partir de valores de uma variável \"***K***\" predefinidos."]},{"cell_type":"markdown","source":["Podemos ver então que temos já definidas as três operações que iremos aplicar: Soma, subtração e multiplicação.\n","\n","A ideia aqui é que iremos dividir essas tarefas entre diferentes processos, onde um estará responsável por inicializar o vetor e delegar as tarefas aos outros três."],"metadata":{"id":"EqyqYZMsWy0G"}},{"cell_type":"markdown","source":["# Tarefa 1"],"metadata":{"id":"uuO60awcZgFq"}},{"cell_type":"markdown","source":["A primeira atividade consiste na distribuição de operações basicas entre os processos, onde teremos um vetor de tamanho determinado pelo usuário no inicio do escopo do programa, que sofrerá alterações a depender do tipo de operação. O código sequencial é dado da seguinte forma:"],"metadata":{"id":"_MRuE8vyZo4d"}},{"cell_type":"code","source":["#include <stdio.h>\n","#define SIZE 12\n","int main (int argc, char **argv){\n","  int i, sum = 0, subtraction = 0, mult = 1;\n","  int array[SIZE];\n","  \n","  for(i = 0; i < SIZE; i++)\n","    array[i] = i + 1;\n","  for(i = 0; i < SIZE; i++)\n","    printf(\"array[%d] = %d\\n\", x, array[x]);\n","  for(i = 0; i < SIZE; i++) {\n","    sum = sum + array[i];\n","    subtraction = subtraction - array[i];\n","    mult = mult * array[i];\n","  }\n","\n","  printf(\"Sum = %d\\n\", sum);\n","  printf(\"Subtraction = %d\\n\", subtraction);\n","  printf(\"Multiply = %d\\n\", mult);\n","  \n","  return 0;\n","}"],"metadata":{"id":"4oa0PUDuWiCW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Então, precisamos aplicar as técnicas de MPI para distribuir e paralelizar as tarefas de soma, subtração e multipicação. Para isso, devemos incluir no cabeçalho a lib mpi.h e inicializar o metodo através da função \"MPI_Init()\" além das variáveis padrões que definem o tamanho do espaço de comunicação dos processos: ***MPI_Comm_Size***, e o id (ou rank) de cada processo: ***MPI_Comm_rank***.\n","\n","Esse espaço de comunicação dado por ***MPI_COMM_WORLD*** é a camada por onde os processos irão se comunicar."],"metadata":{"id":"QCJ17kpPrHQf"}},{"cell_type":"code","source":["#include <stdio.h>\n","#define SIZE 12\n","\n","#include <mpi.h>\n","\n","int main (int argc, char **argv){\n","\n","  int array[SIZE];\n","  int numberOfProcessors, id, to, from, tag = 1000;\n","  ...\n","  \n","  MPI_Init(&argc, &argv);\n","  MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcessors);\n","  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n","  MPI_Status status;\n","  ..."],"metadata":{"id":"nenY6lSurajU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Depois disso, podemos definir o comportamento do processo mestre, que irá iniciar o vetor de inteiros e delegar as funções aos outros processos, além de receber os resultados individuais no final das execuções e imprimi-los na tela."],"metadata":{"id":"3NYnEzKatNVJ"}},{"cell_type":"code","source":["  ...\n","  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n","  MPI_Status status;\n","\n","  switch(id){\n","    case 0:\n","      for(i = 0; i < SIZE; i++){\n","        array[i] = i + 1;\n","        printf(\"%d\\t\", array[i]);\n","      }\n","      printf(\"\\n\");\n","\n","      for(to = 1; to < numberOfProcessors; to++){\n","        MPI_Send(&array, SIZE, MPI_INT, to, tag, MPI_COMM_WORLD);\n","        MPI_Send(&operations[to-1], 1, MPI_CHAR, to, tag, MPI_COMM_WORLD);\n","      }\n","      for(to = 1; to < numberOfProcessors; to++) {\n","        MPI_Recv(&result, 1, MPI_INT, to, tag, MPI_COMM_WORLD, &status);\n","        MPI_Recv(&operationsRec, 1, MPI_CHAR, to, tag, MPI_COMM_WORLD, &status);\n","        printf (\"(%c) = %d\\n\", operationsRec, result);\n","      }\n","      break;\n","    ..."],"metadata":{"id":"89TDjS9Ati_B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como podemos ver, o processo mestre recebe id ou rank = 0, ja que é o primeiro a ser criado. Nele faremos a atribuição dos valores ao vetor de inteiros e o imprimimos na tela.\n","\n","Após isso, partimos para o envio desse vetor para os outros processos através do método ***MPI_Send*** que recebe como parâmetros respectivamente:\n","1. Dados a serem enviados\n","2. Quantidade de dados a serem enviados.\n","3. Tipo dos dados a serem enviados.\n","4. Rank do processo que deve receber os dados.\n","5. Tag personalizada da mensagem.\n","6. Camada de comunicação pela qual os dados serão enviados\n","\n","Aproveitamos e enviamos também as respectivas operações matemáticas que devem ser executadas por cada processo. Por fim, realizamos um novo loop para receber os resultados das operações de cada processo e os imprimimos na tela.\n","\n","O próximo passo então é definir o que cada processo fará com esse vetor."],"metadata":{"id":"kRGoq1Ust3_c"}},{"cell_type":"code","source":["      ...\n","        MPI_Recv(&operationsRec, 1, MPI_CHAR, to, tag, MPI_COMM_WORLD, &status);\n","        printf (\"(%c) = %d\\n\", operationsRec, result);\n","      }\n","      break;\n","\n","    default:\n","      MPI_Recv(&array, SIZE, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);\n","      MPI_Recv(&operationsRec, 1, MPI_CHAR, 0, tag, MPI_COMM_WORLD, &status);\n","\n","      switch(operationsRec){\n","        case '+':\n","          value = 0;\n","          for(i = 0; i < SIZE; i++){\n","            value += array[i];\n","          }\n","          break;\n","\n","        case '-':\n","          value = 0;\n","          for(i = 0; i < SIZE; i++){\n","            value -= array[i];\n","          }\n","          break;\n","\n","        case '*':\n","          value = 1;\n","          for(i = 0; i < SIZE; i++){\n","            value *= array[i];\n","          }\n","          break;        \n","      }\n","      MPI_Send(&value, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);\n","      MPI_Send(&operationsRec, 1, MPI_CHAR, 0, tag, MPI_COMM_WORLD);\n","      break;\n","  }\n","\n","  MPI_Finalize();\n","\n","  return 0;\n","\n","}"],"metadata":{"id":"wMQHjKmTwES9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nesse momento, já que todos os processos irão receber o mesmo vetor, podemos definir um comportamento padrão utilizando o caso ***default*** no switch-case para o recebimento através do método ***MPI_Recv***. Depois disso, apenas criamos um novo switch para cada tipo de operação, à realizamos e depois retornamos o resultado através da ***MPI_Send***.\n","\n","Por fim, finalizamos as operações com MPI utilizando o método ***MPI_Finalize***."],"metadata":{"id":"09-9CZywwbLj"}},{"cell_type":"markdown","source":["# Tarefa 2"],"metadata":{"id":"nzyCnNBYbGSK"}},{"cell_type":"markdown","source":["A segunda atividade segue o mesmo principio de distribuição de tarefas, dessa vez para o calculo de uma função algebrica de terceiro gral, onde cada processo ficará encarregado de  calcular um pedaço da equação.\n","\n","Começamos com os mesmos métodos de ***Init***, definição de ***size*** e de ***rank***. Lembrando sempre de incluir a lib mpi.h e declarar as demais variáveis necessárias para o programa."],"metadata":{"id":"hFwSadAFbJ-4"}},{"cell_type":"code","source":["#include <stdio.h>\n","#include <math.h>\n","#include <mpi.h>\n","\n","int main (int argc, char **argv){\n","\n","  double coefficient[4], total, x, p[3], vars[3], result, w1[2], w2[2], w3[3];\n","  char c;\n","  int numOfProc, id, tag1 = 10, tag2 = 20, tag3 = 30;\n","\n","  MPI_Init(&argc, &argv);\n","  MPI_Comm_size(MPI_COMM_WORLD, &numOfProc);\n","  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n","  MPI_Status status;\n","  ..."],"metadata":{"id":"B5587rLzy7HH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como dessa vez, os processos irão receber valores diferentes com os quais trabalhar, também escolhemos criar tags diferentes para cada processo, apenas para ajudar na distinção dos comandos. Depois que fazemos essa parte inicial podemos continuar para a divisão de trabalho, sempre começando pelo rank do processo mestre.\n","\n","Aqui o processo 0 começa imprimindo a fórmula da equação na tela e pedindo para que o usuário insira cada variavel relacionada a ela. Depois que todos os valores das variáveis são inseridos, os vetores com os dados que devem ser enviados a cada processo são separados e enviados um por um.\n","\n","Por fim, o mestre aguarda o retorno dos valores já calculados por cada processo e os concatena no resultado final, o impimindo na tela."],"metadata":{"id":"L4PldtG60kM2"}},{"cell_type":"code","source":["  ...\n","  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n","  MPI_Status status;\n","\n","  switch(id){\n","    case 0:\n","      printf (\"\\nf(x) = a*x^3 + b*x^2 + c*x + d\\n\");\n","\n","      for(c = 'a'; c < 'e'; c++) {\n","        printf (\"\\nEnter the value of the 'constants' %c:\\n\", c);\n","        scanf (\"%lf\", &coefficient[c - 'a']);\n","      }\n","\n","      printf(\"\\nf(x) = %lf*x^3 + %lf*x^2 + %lf*x + %lf\\n\", coefficient[0], coefficient[1], coefficient[2], coefficient[3]);\n","\n","      printf(\"\\nEnter the value of 'x':\\n\");\n","      scanf(\"%lf\", &x);\n","      \n","      w1[0] = coefficient[0];\n","      w2[0] = coefficient[1];\n","      w3[0] = coefficient[2];\n","      w3[1] = coefficient[3];\n","      w1[1] = w2[1] = w3[2] = x;\n","      \n","      MPI_Send(&w1, 2, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n","      MPI_Send(&w2, 2, MPI_DOUBLE, 2, tag2, MPI_COMM_WORLD);\n","      MPI_Send(&w3, 3, MPI_DOUBLE, 3, tag3, MPI_COMM_WORLD);\n","      \n","      MPI_Recv(&p[0], 1, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD, &status);\n","      MPI_Recv(&p[1], 1, MPI_DOUBLE, 2, tag2, MPI_COMM_WORLD, &status);\n","      MPI_Recv(&p[2], 1, MPI_DOUBLE, 3, tag3, MPI_COMM_WORLD, &status);\n","      \n","      printf(\"\\nf(%lf) = %lf\\n\", x, p[0]+p[1]+p[2]);\n","      break;\n","      ..."],"metadata":{"id":"A7OqG53A1D1D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A atividade executada pelos processos filhos segue um esquema parecido com o da primeira tarefa, onde cada processo calcula uma parte independente da equação e a retorna para o processo mestre.\n","\n","Ao final, também encerramos o MPI com o ***MPI_Finalize***."],"metadata":{"id":"HICGLHKf2RAS"}},{"cell_type":"code","source":["      ...  \n","      printf(\"\\nf(%lf) = %lf\\n\", x, p[0]+p[1]+p[2]);\n","      break;\n","    \n","    case 1:\n","      printf(\"case 1\");\n","      MPI_Recv(&vars, 2, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n","      //result = vars[0] * pow(vars[1], 3);\n","      result = vars[0] * vars[1] * vars[1] * vars[1];\n","      MPI_Send(&result, 1, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD);\n","      break;\n","    case 2:\n","      MPI_Recv(&vars, 2, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD, &status);\n","      //result = vars[0] * pow(vars[1], 2);\n","      result = vars[0] * vars[1] * vars[1];\n","      MPI_Send(&result, 1, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n","      break;\n","    case 3:\n","      MPI_Recv(&vars, 3, MPI_DOUBLE, 0, tag3, MPI_COMM_WORLD, &status);\n","      result = vars[0] * vars[2] + vars[1];\n","      MPI_Send(&result, 1, MPI_DOUBLE, 0, tag3, MPI_COMM_WORLD);\n","      break;\n","  }\n","\n","  MPI_Finalize();\n","  return 0;\n","}"],"metadata":{"id":"lAGnIG-92g6j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tarefa 3"],"metadata":{"id":"WKytTFFKbIUa"}},{"cell_type":"markdown","source":["Por fim, a terceira atividade nos apresenta uma matriz quadrada onde os valores da diagonal principal, superdiagonal e subdiagonal devem sofrer alterações com base em valores de variaveis \"***K***\" pré-definidos. A solução dessa atividade se assemelha bastante à primeira parte desse Hands-On, pois enviaremos a mesma matriz inicializada pelo processo mestre, para cada um dos outros processos, apenas alterando a operação que será executada por cada um.\n","\n","Começamos então com os mesmos métodos de anteriormente, além de aproveitar uma função de impressão de matriz já codificada pelo professor."],"metadata":{"id":"6Uowebg6bQaY"}},{"cell_type":"code","source":["#include <stdio.h>\n","#include <mpi.h>\n","#define ORDER 4\n","\n","void printMatrix (int m[][ORDER]) {\n","  int i, j;\n","  for(i = 0; i < ORDER; i++) {\n","    printf (\"| \");\n","    for (j = 0; j < ORDER; j++) {\n","      printf (\"%3d \", m[i][j]);\n","    }\n","    printf (\"|\\n\");\n","  }\n","  printf (\"\\n\");\n","}\n","\n","int main (int argc, char **argv){\n","\n","  int k[3] = {100, 200, 300};\n","  int matrix[ORDER][ORDER], rcv1[ORDER][ORDER], rcv2[ORDER][ORDER], rcv3[ORDER][ORDER], size = ORDER*ORDER;\n","  int i, j, p, numProc, rank, tag = 10;\n","  \n","  MPI_Init(&argc, &argv);\n","  MPI_Comm_size(MPI_COMM_WORLD, &numProc);\n","  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n","  MPI_Status status;\n","  ..."],"metadata":{"id":"OoyOxsrK4U-0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Passamos então para o rank 0 que fica responsável por inicializar a matriz e a enviar para os outros processos, como de costume. Mas dessa vez, na hora da recepção, faremos atribuições distintas com base nas matrizes recebidas de cada processo.\n","\n","Nesse caso, cada rank ficou responsável por uma diagonal, sendo elas a ***principal***, a ***superdiagonal*** e a ***subdiagonal***, da matriz quadrada. Com isso, a mensagem de resposta de cada processo contem uma matriz identica à original, com exceção da diagonal a qual o mesmo ficou encarregado.\n","\n","Quando a resposta dos ranks filhos é recebida, o processo mestre roda um único loop passando por cada posição da matriz e apenas reatribui os novos valores às suas respectivas diagonais."],"metadata":{"id":"E6cBqMxe4l2v"}},{"cell_type":"code","source":["  ...\n","  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n","  MPI_Status status;\n","\n","  switch(rank){\n","    case 0:\n","      for(i = 0; i < ORDER; i++) {\n","        for(j = 0; j < ORDER; j++) {\n","          if( i == j )\n","            matrix[i][j] = i + j +1;\n","          else if(i == (j + 1)) {\n","            matrix[i][j] = i +  j + 1;\n","            matrix[j][i] = matrix[i][j];\n","          } else\n","           matrix[i][j] = 0;\n","        }\n","      }\n","      printMatrix(matrix);\n","      for(p = 1; p < numProc; p++){\n","        MPI_Send(&matrix, size, MPI_INT, p, tag, MPI_COMM_WORLD);\n","      }\n","      MPI_Recv(&rcv1, size, MPI_INT, 1, tag, MPI_COMM_WORLD, &status);\n","      MPI_Recv(&rcv2, size, MPI_INT, 2, tag, MPI_COMM_WORLD, &status);\n","      MPI_Recv(&rcv3, size, MPI_INT, 3, tag, MPI_COMM_WORLD, &status);\n","      for(i = 0; i < ORDER; i++){\n","        matrix[i][i] = rcv1[i][i];\n","        matrix[i + 1][i] = rcv2[i + 1][i];\n","        matrix[i][i + 1] = rcv3[i][i + 1];    \n","      }\n","      printMatrix(matrix);\n","      break;\n","      ..."],"metadata":{"id":"yzMs7vdJ602s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para finalizar, criamos um case ***default*** para os ranks filhos, ja que todos recebem a mesma matriz. Um único loop roda em cada processo e apenas nas posições designadas a eles, os mesmos executam a operação de soma com os respecivos valores de ***K***, retornando a matriz modificada para o processo mestre logo após isso.\n","\n","Por fim, mais uma vez, encerramos o MPI."],"metadata":{"id":"79wwE5ly7Nrk"}},{"cell_type":"code","source":["      ...\n","      printMatrix(matrix);\n","      break;\n","\n","    default:\n","      MPI_Recv(&matrix, size, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);\n","\n","      for(i = 0; i < ORDER; i++){\n","        switch(rank){\n","          case 1:\n","            matrix[i][i] += k[0];\n","            break;\n","          case 2:\n","            matrix[i + 1][i] += k[1];\n","            break;\n","          case 3:\n","            matrix[i][i + 1] += k[2];\n","            break;\n","        }\n","      }\n","      MPI_Send(&matrix, size, MPI_INT, 0, tag, MPI_COMM_WORLD);\n","      break;\n","  }\n","\n","  MPI_Finalize();\n","  return 0;\n","}"],"metadata":{"id":"XrGTCB_l7xgI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EkecWU4KKvo"},"source":["# Conclusões"]},{"cell_type":"markdown","metadata":{"id":"EOXpKoRrJ5sm"},"source":["Podemos notar claramente a semelhança entre as três atividades praticadas por esse Hands-On, o que ajuda bastante a se acostumar com os métodos e variáveis utilizados pelo MPI. Apesar de semelhantes, cada atividade exigiu uma estratégia diferente para o envio e recebimento das mensagens por cada processo, o que também auxilia no aprendizado de uso da ferramenta."]},{"cell_type":"markdown","metadata":{"id":"qbxAR_nZ3ey8"},"source":["# Referencias"]},{"cell_type":"markdown","metadata":{"id":"eHs1Te143ezA"},"source":["[1] M. Boratto. Hands-On Supercomputing with Parallel Computing. Available: https://github.com/ muriloboratto/Hands-On-Supercomputing-with-Parallel-Computing. 2022.\n","\n","[2] Forum, Message Passing Interface. MPI: A Message-Passing Interface Standard. University of Tennessee,\n","1994, USA."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["-T1_3R6iCaFL","BF4BYJbRK3dk","gLAY9cBjxrjs","0MiGCjRsrn9m","tODCNPjB9eGn","u9P77BloNxg7","MNXuoP6ROAvo","GmH0n7r1PVbM","L5kUBzMXvW3z","HYIuvShRPBvo","7-PpumH1KDAH","xbA7sVD4x_Mj"],"provenance":[{"file_id":"https://github.com/jupyter-papers/mock-paper/blob/master/FerroicBlocks_mockup_paper_v3a.ipynb","timestamp":1659115650450}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}